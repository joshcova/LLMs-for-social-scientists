{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Hh_tFLIiwqDZHIRpRGlFAXx5iL1Joxjt",
      "authorship_tag": "ABX9TyNyq8vIEVCRaKtc8FwgDNe1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshcova/LLMs-for-social-scientists/blob/main/code/03_ChatGPT_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will go over how to classify the same texts that we have classified in our classifiers.ipynb notebook, but this time using OpenAI's API.\n",
        "\n",
        "Note, that to run this notebook, you will first have to set up a [API key](https://platform.openai.com/docs/api-reference/introduction) and save it locally.\n",
        "\n",
        "It is important that you do not share the API key with others."
      ],
      "metadata": {
        "id": "w8btiKDQ_rB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install retry"
      ],
      "metadata": {
        "id": "1Z1iN90qMMui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1QxZtM8J0tx"
      },
      "outputs": [],
      "source": [
        "# Load necessary libraries\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "from typing import List, Dict\n",
        "from retry.api import retry_call\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up an API key"
      ],
      "metadata": {
        "id": "jqDWb_r8Oy-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WARNING: Use this method only for testing and personal projects\n",
        "# Set up your API key (https://platform.openai.com/docs/quickstart)\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get(\"\")) # This is where you would insert your personal API key"
      ],
      "metadata": {
        "id": "dnpFq_cVMW-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading our classification sample"
      ],
      "metadata": {
        "id": "gw4c1q0YOx_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us first load our dataset of UK newspaper headlines. Note that as interacting with OpenAI's API comes is not free, we will only conduct this analysis on a sample of the corpus of newspaper headlines."
      ],
      "metadata": {
        "id": "8SN4o5yrAYfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/joshcova/LLMs-for-social-scientists/main/data/uk_media_2.csv\")\n",
        "\n",
        "df = df[[\"majortopic\",\"text\"]]\n",
        "df = df.rename(columns={\"majortopic\":\"label\"})\n",
        "\n",
        "df = df.groupby(\"label\").sample(n=50, random_state=1)"
      ],
      "metadata": {
        "id": "em2YSB5ONh7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting the LLM"
      ],
      "metadata": {
        "id": "gyTrXjd1NoUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Open AI's libraries\n",
        "\n",
        "import openai\n",
        "from openai import OpenAIError\n",
        "\n",
        "# Here we create a function to automate our API requests\n",
        "\n",
        "def send_prompt_with_context(model: str,\n",
        "                             messages: List[Dict],\n",
        "                             max_tokens: int = 0) -> Dict[str, str]:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.0, # Low temperatures results into more deterministic outputs, high temperatures result into more stochastic outputs\n",
        "        seed=42, # Setting the same seed is important for reproducibility\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "    pass\n",
        "\n",
        "categories = [\"0: Law & Crime\", \"1: Macroeconomics\", \"2: Unclear\"]\n",
        "\n",
        "definitions = \"\"\"\n",
        "0: The newspaper headline concerns a topic other than Macroeconomics or Law & Crime.\n",
        "1: The newspaper headline concerns the topic Macroeconomics. \\\\\n",
        "2: It is unclear whether the newspaper headline concerns the topics Law & Crime or Macroeconomics.\n",
        "\"\"\"\n",
        "\n",
        "def predict_sentiment(review: str, model: str) -> Dict[str, str]:\n",
        "                system_msg = f\"\"\"\n",
        "                    You are a skilled research assistant who will help to classify newspaper headlines. \\\\\n",
        "                    Classify the following text into one of the given categories: {categories}\\n{definitions} \\\\\n",
        "                    Only include the number of the selected category in your response and no further text.\"\n",
        "                    \"\"\"\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": system_msg},\n",
        "                    {\"role\": \"user\", \"content\": review}\n",
        "                ]\n",
        "                return send_prompt_with_context(model, messages)"
      ],
      "metadata": {
        "id": "WPOu4J4RNpT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sending out the texts"
      ],
      "metadata": {
        "id": "Z87FSuUWO63C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where we interact with the API. Note that you can choose from different GPT models.\n",
        "\n",
        "df['GPT_4o'] = df['text'].progress_apply(lambda x: predict_sentiment(x, model='gpt-4o-2024-11-20'))"
      ],
      "metadata": {
        "id": "od5KZEXXNtQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reviewing a subset of annotated sentences"
      ],
      "metadata": {
        "id": "qX1RLFaHO9bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['text', 'GPT_4o']])"
      ],
      "metadata": {
        "id": "KO6sBrNZNw4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To run our performance metrics, we first need to ensure that the data is saved in the same type\n",
        "\n",
        "df[\"GPT_4o\"] = df[\"GPT_4o\"].astype(int)"
      ],
      "metadata": {
        "id": "Zos0FqLBNyxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "QuTFBny8PBVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
        "\n",
        "metrics = {\n",
        "    \"Metric\": [\"F1 Score (macro)\", \"F1 Score (micro)\", \"Balanced Accuracy\"],\n",
        "    \"Value\": [\n",
        "        f1_score(df[\"label\"], df[\"GPT_4o\"], average='macro'),\n",
        "        f1_score(df[\"label\"], df[\"GPT_4o\"], average='micro'),\n",
        "        balanced_accuracy_score(df[\"label\"], df[\"GPT_4o\"])\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert the dictionary into a DataFrame for nice tabular representation\n",
        "results_df = pd.DataFrame(metrics)\n",
        "\n",
        "# Display the results table\n",
        "results_df"
      ],
      "metadata": {
        "id": "rNXr4kmxMtKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Metric</th>\n",
        "      <th>Value</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>F1 Score (macro)</td>\n",
        "      <td>0.727384</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>F1 Score (micro)</td>\n",
        "      <td>0.746667</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>Balanced Accuracy</td>\n",
        "      <td>0.746667</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
      ],
      "metadata": {
        "id": "jq73aZ051kdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating metrics per class\n",
        "# Replace the second df with any model of your choice\n",
        "precision_per_class = precision_score(df[\"label\"], df[\"GPT_4o\"], average=None, labels=[0,1,2])\n",
        "recall_per_class = recall_score(df[\"label\"], df[\"GPT_4o\"], average=None, labels=[0,1,2])\n",
        "f1_per_class = f1_score(df[\"label\"], df[\"GPT_4o\"], average=None, labels=[0,1,2])\n",
        "\n",
        "# Since accuracy is a global metric (not class-specific), we will not recalculate it here.\n",
        "\n",
        "# Create a DataFrame from the metrics\n",
        "metrics_per_class_df = pd.DataFrame({\n",
        "    \"Class\": [0, 1, 2],\n",
        "    \"Precision\": precision_per_class,\n",
        "    \"Recall\": recall_per_class,\n",
        "    \"F1 Score\": f1_per_class\n",
        "})\n",
        "\n",
        "# Display the results table\n",
        "metrics_per_class_df"
      ],
      "metadata": {
        "id": "OGRYFi9HMxI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Class</th>\n",
        "      <th>Precision</th>\n",
        "      <th>Recall</th>\n",
        "      <th>F1 Score</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>0</td>\n",
        "      <td>0.863636</td>\n",
        "      <td>0.38</td>\n",
        "      <td>0.527778</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>1</td>\n",
        "      <td>0.920000</td>\n",
        "      <td>0.92</td>\n",
        "      <td>0.920000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>2</td>\n",
        "      <td>0.602564</td>\n",
        "      <td>0.94</td>\n",
        "      <td>0.734375</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
      ],
      "metadata": {
        "id": "q5NaLOT2DtqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For future reference, it is advisable to save the resulting dataframe locally\n",
        "\n",
        "df.to_csv(\"uk_media_2_results.csv\")"
      ],
      "metadata": {
        "id": "Wc40tRG2a_DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Central bank independence corpus\n",
        "\n",
        "Let us see how well ChatGPT performs on our other dataset on parliamentary interventions on central bank independence.  "
      ],
      "metadata": {
        "id": "mL4Rj3ICPPhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cbi = pd.read_csv(\"https://raw.githubusercontent.com/joshcova/LLMs-for-social-scientists/main/data/uk_cbi_sample.csv\")"
      ],
      "metadata": {
        "id": "m1PUp_-majBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAIError\n",
        "\n",
        "def send_prompt_with_context(model: str,\n",
        "                             messages: List[Dict],\n",
        "                             max_tokens: int = 0) -> Dict[str, str]:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.0,\n",
        "        seed=42,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "    pass\n",
        "\n",
        "categories = [\"0: anti-independence\", \"1: pro-independence\", \"2: unrelated\"]\n",
        "\n",
        "definitions = \"\"\"\n",
        "0: The statement expresses opposition for central bank independence for the Bank of England. \\\\\n",
        "1: The statement expresses support for central bank independence for the Bank of England. \\\\\n",
        "2: The statement does not contain a clear expression in support or opposition to Bank of England central bank independence or is on an unrelated topic\n",
        "(e.g. on the European Central Bank).\n",
        "\"\"\"\n",
        "\n",
        "def predict_sentiment(review: str, model: str) -> Dict[str, str]:\n",
        "                system_msg = f\"\"\"\n",
        "                    You are a skilled research assistant who will help to classify parliamentary interventions on central bank independence. \\\\\n",
        "                    Central bank independence can relate to formal independence (the legal provisions that guarantee the central bank's autonomy, such as its mandate, its organizational structure, and the procedures for appointing its leaders), and actual independence (taking into account factors such as its political and institutional environment, its relationship with the government, and the level of transparency and accountability in its operations). \\\\\n",
        "                    Classify the following text into one of the given categories: {categories}\\n{definitions} \\\\\n",
        "                    Only include the number of the selected category in your response and no further text.\"\n",
        "                    \"\"\"\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": system_msg},\n",
        "                    {\"role\": \"user\", \"content\": review}\n",
        "                ]\n",
        "                return send_prompt_with_context(model, messages)\n"
      ],
      "metadata": {
        "id": "llR6beiTQCkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cbi['GPT_4o'] = df_cbi['sents'].progress_apply(lambda x: predict_sentiment(x, model='gpt-4o-2024-11-20'))"
      ],
      "metadata": {
        "id": "C-bjqnaYQDOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cbi[\"GPT_4o\"] = df_cbi[\"GPT_4o\"].astype(int)"
      ],
      "metadata": {
        "id": "UX_GoeHnFPhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
        "\n",
        "metrics = {\n",
        "    \"Metric\": [\"F1 Score (macro)\", \"F1 Score (micro)\", \"Balanced Accuracy\"],\n",
        "    \"Value\": [\n",
        "        f1_score(df_cbi[\"results_number\"], df_cbi[\"GPT_4o\"], average='macro'),\n",
        "        f1_score(df_cbi[\"results_number\"], df_cbi[\"GPT_4o\"], average='micro'),\n",
        "        balanced_accuracy_score(df_cbi[\"results_number\"], df_cbi[\"GPT_4o\"])\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert the dictionary into a DataFrame for nice tabular representation\n",
        "results_df_cbi = pd.DataFrame(metrics)\n",
        "\n",
        "# Display the results table\n",
        "results_df_cbi"
      ],
      "metadata": {
        "id": "qX5sPYkaFRSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Metric</th>\n",
        "      <th>Value</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>F1 Score (macro)</td>\n",
        "      <td>0.622342</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>F1 Score (micro)</td>\n",
        "      <td>0.726667</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>Balanced Accuracy</td>\n",
        "      <td>0.701783</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
      ],
      "metadata": {
        "id": "d7BKx10PFTBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating metrics per class\n",
        "# Replace the second df with any model of your choice\n",
        "precision_per_class_cbi = precision_score(df_cbi[\"results_number\"], df_cbi[\"GPT_4o\"], average=None, labels=[0,1,2])\n",
        "recall_per_class_cbi = recall_score(df_cbi[\"results_number\"], df_cbi[\"GPT_4o\"], average=None, labels=[0,1,2])\n",
        "f1_per_class_cbi = f1_score(df_cbi[\"results_number\"], df_cbi[\"GPT_4o\"], average=None, labels=[0,1,2])\n",
        "\n",
        "# Since accuracy is a global metric (not class-specific), we will not recalculate it here.\n",
        "\n",
        "# Create a DataFrame from the metrics\n",
        "metrics_per_class_df_cbi = pd.DataFrame({\n",
        "    \"Class\": [0, 1, 2],\n",
        "    \"Precision\": precision_per_class_cbi,\n",
        "    \"Recall\": recall_per_class_cbi,\n",
        "    \"F1 Score\": f1_per_class_cbi\n",
        "})\n",
        "\n",
        "# Display the results table\n",
        "metrics_per_class_df_cbi"
      ],
      "metadata": {
        "id": "bo2SKbxKFUMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Class</th>\n",
        "      <th>Precision</th>\n",
        "      <th>Recall</th>\n",
        "      <th>F1 Score</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>0</td>\n",
        "      <td>0.225806</td>\n",
        "      <td>0.700000</td>\n",
        "      <td>0.341463</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>1</td>\n",
        "      <td>0.897436</td>\n",
        "      <td>0.823529</td>\n",
        "      <td>0.858896</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>2</td>\n",
        "      <td>0.780488</td>\n",
        "      <td>0.581818</td>\n",
        "      <td>0.666667</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
      ],
      "metadata": {
        "id": "e5m7KLVtFWya"
      }
    }
  ]
}